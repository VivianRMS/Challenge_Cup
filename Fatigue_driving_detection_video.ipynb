{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "74be1650-ed5f-4bdf-bfa2-30b26c4f38b3",
   "metadata": {},
   "source": [
    "# “挑战杯”大赛“揭榜挂帅”赛道-疲劳分神驾驶检测Baseline\n",
    "本案例的第1、2步可以省略，第3步的文件夹中已包含处理好的数据集，如果不需要训练，可以直接运行步骤4<br>\n",
    "注意：该案例中moxing库为华为自研，无法在本地安装。<br>\n",
    "如需在本地使用案例代码，可以把代码拷贝之后 下载到本地使用<br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "abbfdff0-9054-490e-9d91-34c502ab782f",
   "metadata": {},
   "source": [
    "## 1、视频抽帧\n",
    "抽取关键帧图像使用代码如下，可能需要修改视频所在路径\n",
    "\n",
    "这里只用了一开始提供的142段视频"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "caf965ba-eef6-4582-8ff4-bedf44479f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import errno\n",
    "\n",
    "\n",
    "\n",
    "def getInfo(sourcePath):\n",
    "    cap = cv2.VideoCapture(sourcePath)\n",
    "    info = {\n",
    "        \"framecount\": cap.get(cv2.CAP_PROP_FRAME_COUNT),\n",
    "        \"fps\": cap.get(cv2.CAP_PROP_FPS),\n",
    "        \"width\": int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)),\n",
    "        \"heigth\": int(cap.get(cv2.CAP_PROP_FRAME_Heigth)),\n",
    "        \"codec\": int(cap.get(cv2.CAP_PROP_FOURCC))\n",
    "    }\n",
    "    cap.release()\n",
    "    return info\n",
    "\n",
    "\n",
    "def scale(img, xScale, yScale):\n",
    "    res = cv2.resize(img, None,fx=xScale, fy=yScale, interpolation = cv2.INTER_AREA)\n",
    "    return res\n",
    "\n",
    "\n",
    "def resize(img, width, heigth):\n",
    "    res = cv2.resize(img, (width, heigth), interpolation=cv2.INTER_AREA)\n",
    "    return res\n",
    "\n",
    "#可能的优化：\n",
    "#1.K-means clustering的可靠程度\n",
    "#2.提取色块个数\n",
    "#3.是否应该全局提取色块个数？\n",
    "\n",
    "#对一张keyframe image用k-means clustering完成主要色块的bgr值和各色块在image中拥有的pixel个数\n",
    "#返回cols:a list of {\"count\": 各色块在image中拥有的pixel个数, \"col\": 该色块的bgr值}\n",
    "def extract_cols(image, numCols):\n",
    "    # convert to np.float32 matrix that can be clustered\n",
    "    Z = image.reshape((-1, 3))\n",
    "    Z = np.float32(Z)\n",
    "\n",
    "    #k-means clustering\n",
    "    # Set parameters for the clustering\n",
    "    max_iter = 20\n",
    "    epsilon = 1.0\n",
    "    K = numCols #cluster centroid个数\n",
    "    criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, max_iter, epsilon)\n",
    "    labels = np.array([]) #每个pixel属于哪个色块的编号的ordered list\n",
    "    # cluster\n",
    "    compactness, labels, centers = cv2.kmeans(Z, K, labels, criteria, 10, cv2.KMEANS_RANDOM_CENTERS)\n",
    "    #centers是\n",
    "\n",
    "    clusterCounts = []#统计属于每一个色块的pixel个数的list\n",
    "    for idx in range(K):\n",
    "        count = len(Z[np.where(labels == idx)]) #属于某个色块的pixel个数\n",
    "        clusterCounts.append(count)\n",
    "\n",
    "    rgbCenters = []\n",
    "    for center in centers:\n",
    "        bgr = center.tolist()\n",
    "        bgr.reverse()\n",
    "        rgbCenters.append(bgr)\n",
    "\n",
    "    cols = []\n",
    "    for i in range(K):\n",
    "        iCol = {\n",
    "            \"count\": clusterCounts[i],\n",
    "            \"col\": rgbCenters[i]\n",
    "        }\n",
    "        cols.append(iCol)\n",
    "\n",
    "    return cols\n",
    "\n",
    "\n",
    "#可能的优化：\n",
    "#1.提取灰度信息可靠吗？\n",
    "#2.提取灰度信息后\n",
    "###1.缩放图片至四分之一会产生什么影响？\n",
    "###2.对缩放后的gray进行了高斯模糊会产生什么影响？\n",
    "#3.逐帧比较灰度点不同个数多少可以判定为\n",
    "\n",
    "#返还data\n",
    "#data组成：\n",
    "##1.\"frame_info\": a list of {\"frame_number\",\"与上一帧灰度像素点不同个数(diff_count)包括0个\"}\n",
    "##2.\"stats\":{\n",
    "'''\n",
    "        \"num\": 帧数,\n",
    "        \"min\": 灰度差异个数最小值,\n",
    "        \"max\": 灰度差异个数最大值,\n",
    "        \"mean\": 灰度差异个数平均值,\n",
    "        \"median\": 灰度差异个数中位数,\n",
    "        \"sd\": 灰度差异个数标准差,\n",
    "        \"greater_than_mean\": 灰度差异个数比平均值大的帧数,\n",
    "        \"greater_than_median\": 灰度差异个数比中位数大的帧数,\n",
    "        \"greater_than_one_sd\": 灰度差异个数超出1个std的帧数,\n",
    "        \"greater_than_two_sd\": 灰度差异个数超出2个std的帧数,\n",
    "        \"greater_than_three_sd\": 灰度差异个数超出3个std的帧数,\n",
    "'''\n",
    "### }\n",
    "def calculateFrameStats(sourcePath, verbose=True, after_frame=0):  # 提取相邻帧的差别\n",
    "\n",
    "    cap = cv2.VideoCapture(sourcePath)  # 提取视频\n",
    "\n",
    "    data = {\n",
    "        \"frame_info\": []\n",
    "    }\n",
    "\n",
    "    lastFrame = None\n",
    "    while(cap.isOpened()):\n",
    "        ret, frame = cap.read()\n",
    "        if frame is None:\n",
    "            break\n",
    "\n",
    "        frame_number = cap.get(cv2.CAP_PROP_POS_FRAMES) - 1\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)      # 提取灰度信息\n",
    "        gray = scale(gray, 0.25, 0.25)      # 缩放为原来的四分之一\n",
    "        gray = cv2.GaussianBlur(gray, (9, 9), 0.0)   # 做高斯模糊\n",
    "        # lastFrame = gray\n",
    "        if frame_number < after_frame:\n",
    "            lastFrame = gray\n",
    "            continue\n",
    "\n",
    "        if lastFrame is not None:\n",
    "            diff = cv2.subtract(gray, lastFrame)        # 用当前帧减去上一帧\n",
    "            diffMag = cv2.countNonZero(diff)        # 计算两帧灰度值不同的像素点个数\n",
    "            frame_info = {\n",
    "                \"frame_number\": int(frame_number),\n",
    "                \"diff_count\": int(diffMag)\n",
    "            }\n",
    "            data[\"frame_info\"].append(frame_info)\n",
    "            if verbose:\n",
    "                cv2.imshow('diff', diff)\n",
    "                if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                    break\n",
    "        # Keep a ref to this frame for differencing on the next iteration\n",
    "        lastFrame = gray\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    # compute some states\n",
    "    diff_counts = [fi[\"diff_count\"] for fi in data[\"frame_info\"]]\n",
    "    data[\"stats\"] = {\n",
    "        \"num\": len(diff_counts),\n",
    "        \"min\": np.min(diff_counts),\n",
    "        \"max\": np.max(diff_counts),\n",
    "        \"mean\": np.mean(diff_counts),\n",
    "        \"median\": np.median(diff_counts),\n",
    "        \"sd\": np.std(diff_counts)   # 计算所有帧之间, 像素变化个数的标准差\n",
    "    }\n",
    "    greater_than_mean = [fi for fi in data[\"frame_info\"] if fi[\"diff_count\"] > data[\"stats\"][\"mean\"]]\n",
    "    greater_than_median = [fi for fi in data[\"frame_info\"] if fi[\"diff_count\"] > data[\"stats\"][\"median\"]]\n",
    "    greater_than_one_sd = [fi for fi in data[\"frame_info\"] if fi[\"diff_count\"] > data[\"stats\"][\"sd\"] + data[\"stats\"][\"mean\"]]\n",
    "    greater_than_two_sd = [fi for fi in data[\"frame_info\"] if fi[\"diff_count\"] > (data[\"stats\"][\"sd\"] * 2) + data[\"stats\"][\"mean\"]]\n",
    "    greater_than_three_sd = [fi for fi in data[\"frame_info\"] if fi[\"diff_count\"] > (data[\"stats\"][\"sd\"] * 3) + data[\"stats\"][\"mean\"]]\n",
    "\n",
    "    # 统计其他信息\n",
    "    data[\"stats\"][\"greater_than_mean\"] = len(greater_than_mean)\n",
    "    data[\"stats\"][\"greater_than_median\"] = len(greater_than_median)\n",
    "    data[\"stats\"][\"greater_than_one_sd\"] = len(greater_than_one_sd)\n",
    "    data[\"stats\"][\"greater_than_three_sd\"] = len(greater_than_three_sd)\n",
    "    data[\"stats\"][\"greater_than_two_sd\"] = len(greater_than_two_sd)\n",
    "\n",
    "    return data\n",
    "\n",
    "def writeImagePyramid(destPath,  name, seqNumber, image):\n",
    "    fullPath = os.path.join(destPath, name + \"_\" + str(seqNumber) + \".png\")\n",
    "    cv2.imwrite(fullPath, image)\n",
    "\n",
    "#可能的优化\n",
    "#1.灰度差异个数threshold取了mean+2.05*std可靠吗？\n",
    "#2.喂给extract_cols的一帧的数据是放缩到了长宽都是100有影响吗？\n",
    "###找的色块个数是5个合理吗？\n",
    "\n",
    "#返还data\n",
    "#给灰度差异个数超过threshold的所有keyframe添加了\"dominant_cols\"变成 {\"frame_number\",\"灰度像素点不同个数(diff_count)包括0个\",\"dominant_cols\"}\n",
    "\n",
    "#把所有keyframe全部存进key frame/images 命名：\"day_man_001_10_1_202.png\"\n",
    "\n",
    "def detectScenes(sourcePath, destPath, data, name, verbose=False):\n",
    "    destDir = os.path.join(destPath, \"images\")\n",
    "\n",
    "    # TODO make sd multiplier externally configurable\n",
    "    # diff_threshold = (data[\"stats\"][\"sd\"] * 1.85) + data[\"stats\"][\"mean\"]\n",
    "    diff_threshold = (data[\"stats\"][\"sd\"] * 2.05) + (data[\"stats\"][\"mean\"]) #difference比均值不超过2.05个std\n",
    "\n",
    "    cap = cv2.VideoCapture(sourcePath)\n",
    "    for index, fi in enumerate(data[\"frame_info\"]):\n",
    "        if fi[\"diff_count\"] < diff_threshold:\n",
    "            continue\n",
    "\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, fi[\"frame_number\"])\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # extract dominant color\n",
    "        small = resize(frame, 100, 100)\n",
    "        cols = extract_cols(small, 5)\n",
    "        data[\"frame_info\"][index][\"dominant_cols\"] = cols\n",
    "\n",
    "        if frame is not None:\n",
    "            # file_name = sourcePath.split('.')[0]\n",
    "            writeImagePyramid(destDir, name, fi[\"frame_number\"], frame)\n",
    "\n",
    "            if verbose:\n",
    "                cv2.imshow('extract', frame)\n",
    "                if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                    break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    return data\n",
    "\n",
    "\n",
    "def makeOutputDirs(path):\n",
    "    try:\n",
    "        os.makedirs(os.path.join(path, \"metadata\"))\n",
    "        os.makedirs(os.path.join(path, \"images\"))\n",
    "\n",
    "    except OSError as exc: # Python >2.5\n",
    "        if exc.errno == errno.EEXIST and os.path.isdir(path):\n",
    "            pass\n",
    "        else:\n",
    "            raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f978ae-f41e-4872-8871-3c4fcb05e46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import moxing as mox\n",
    "#mox.file.copy_parallel('obs://obs-aigallery-zc/clf/dataset/Fatigue_driving_detection_video','Fatigue_driving_detection_video')  # 拷贝142段视频到ModelArts Notebook"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "71f6dde6-c4e5-4029-9c69-94b1c12bd465",
   "metadata": {},
   "source": [
    "抽帧 需要一段时间，请耐心等待"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "847c46a5-5c79-42b5-88f2-c1203412ffe8",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m name \u001b[39m=\u001b[39m filename\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m)[\u001b[39m0\u001b[39m]\n\u001b[1;32m     13\u001b[0m data \u001b[39m=\u001b[39m calculateFrameStats(source, \u001b[39mFalse\u001b[39;00m, \u001b[39m0\u001b[39m)\n\u001b[0;32m---> 14\u001b[0m data \u001b[39m=\u001b[39m detectScenes(source, dest, data, name, \u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m     15\u001b[0m keyframeInfo \u001b[39m=\u001b[39m [frame_info \u001b[39mfor\u001b[39;00m frame_info \u001b[39min\u001b[39;00m data[\u001b[39m\"\u001b[39m\u001b[39mframe_info\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mdominant_cols\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m frame_info]\n\u001b[1;32m     17\u001b[0m \u001b[39m# Write out the results\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[1], line 167\u001b[0m, in \u001b[0;36mdetectScenes\u001b[0;34m(sourcePath, destPath, data, name, verbose)\u001b[0m\n\u001b[1;32m    164\u001b[0m             \u001b[39mif\u001b[39;00m cv2\u001b[39m.\u001b[39mwaitKey(\u001b[39m1\u001b[39m) \u001b[39m&\u001b[39m \u001b[39m0xFF\u001b[39m \u001b[39m==\u001b[39m \u001b[39mord\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mq\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m    165\u001b[0m                 \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m--> 167\u001b[0m cap\u001b[39m.\u001b[39;49mrelease()\n\u001b[1;32m    168\u001b[0m cv2\u001b[39m.\u001b[39mdestroyAllWindows()\n\u001b[1;32m    169\u001b[0m \u001b[39mreturn\u001b[39;00m data\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#对每一个视频进行\n",
    "#1.calculateFrameStats =>得到基本data（针对每一帧得到与上一帧灰度像素点不同个数(diff_count)，并得到对于所有帧diff_count的一些stats量)\n",
    "#2.detectScenes =>在基本data的基础上，基于diff_count>threshold得到keyframe并给每个keyframe进行5个色块提取，得到各个色块含pixel个数和bgr值存在dominant_cols中\n",
    "\n",
    "#建立key frame文件夹\n",
    "#key frame：\n",
    "##1.images文件夹：\n",
    "####1.文件名字：day_man_001_10_1_202.png\n",
    "######内容：所有视频的所有keyframe images\n",
    "##2.metadata文件夹：\n",
    "####1.文件名字：day_man_001_10_1-keyframe-meta.txt\n",
    "######内容：data全部\n",
    "####2.文件名字：day_man_001_10_1-meta.txt\n",
    "######内容：keyframeInfo\n",
    "\n",
    "dest = \"key frame\" # 抽取图像保存路径\n",
    "makeOutputDirs(dest)\n",
    "test_path ='../Fatigue_driving_detection_video'   # 在这里修改视频路径\n",
    "filenames = os.listdir(test_path)\n",
    "count = 0\n",
    "for filename in filenames:\n",
    "\n",
    "    source = os.path.join(test_path, filename)\n",
    "\n",
    "    name = filename.split('.')[0]\n",
    "\n",
    "    data = calculateFrameStats(source, False, 0)\n",
    "    data = detectScenes(source, dest, data, name, False)\n",
    "    keyframeInfo = [frame_info for frame_info in data[\"frame_info\"] if \"dominant_cols\" in frame_info]\n",
    "\n",
    "    # Write out the results\n",
    "\n",
    "    data_fp = os.path.join(dest, \"metadata\", name + \"-meta.txt\")\n",
    "    with open(data_fp, 'w') as f:\n",
    "        f.write(str(data))\n",
    "\n",
    "    keyframe_info_fp = os.path.join(dest, \"metadata\", name + \"-keyframe-meta.txt\")\n",
    "    with open(keyframe_info_fp, 'w') as f:\n",
    "        f.write(str(keyframeInfo))\n",
    "print(count)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bc23d855-6b93-4b14-975a-39db910674e7",
   "metadata": {},
   "source": [
    "## 2、进行目标检测图像标注\n",
    "我使用的是ModelArts标注 <br>\n",
    "2.1 将上面的关键帧图像拷贝到你自己的OBS桶文件目录A中 <br>\n",
    "2.2 点击[此处](https://console.huaweicloud.com/modelarts/?region=cn-north-4#/dataset)进入ModelArts数据管理页面，新建物体检测数据集，数据输入选择上述的OBS桶文件目录A，输出任意选择OBS桶内其他目录B <br>\n",
    "2.3 开始标注，标注人脸标签face和手机标签phone（我标注的时候包括车内所有人脸），标注一部分（如200张图像）即可 <br>\n",
    "2.4 开始智能标注，智能标注需要等待几十分钟，将智能标注后的图像进行核对确认 <br>\n",
    "2.5 导出所有标注完的图像到OBS桶文件夹C中（尽量和A、B区分开） <br>\n",
    "2.6 将OBS桶文件夹C中图像拷贝到ModelArts Notebook进行转换（因为训练所需为TXT标注格式、标注转换代码如下）<br>\n",
    "2.7 划分训练集、验证集、测试集（划分代码如下）<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4731068-93fb-4b06-9a9a-6e6ac72c8f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "'***转换xml标注文件为txt格式，无法直接运行***'\n",
    "import copy\n",
    "from lxml.etree import Element, SubElement, tostring, ElementTree\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "import pickle\n",
    "import os\n",
    "from os import listdir, getcwd\n",
    "from os.path import join\n",
    "\n",
    "classes = [\"face\", \"phone\"]  # 类别\n",
    "\n",
    "\n",
    "def convert(size, box):\n",
    "    dw = 1. / size[0]\n",
    "    dh = 1. / size[1]\n",
    "    x = (box[0] + box[1]) / 2.0\n",
    "    y = (box[2] + box[3]) / 2.0\n",
    "    w = box[1] - box[0]\n",
    "    h = box[3] - box[2]\n",
    "    x = x * dw\n",
    "    w = w * dw\n",
    "    y = y * dh\n",
    "    h = h * dh\n",
    "    return (x, y, w, h)\n",
    "\n",
    "\n",
    "def convert_annotation(image_id):\n",
    "    in_file = open('./label_xml/%s.xml' % (image_id), encoding='UTF-8')\n",
    "\n",
    "    out_file = open('yolov7/datasets/Fatigue_driving_detection/txt_labels/%s.txt' % (image_id), 'w')  # 生成txt格式文件, 保存在yolov7训练所需的数据集路径中\n",
    "    tree = ET.parse(in_file)\n",
    "    root = tree.getroot()\n",
    "    size = root.find('size')\n",
    "    w = int(size.find('width').text)\n",
    "    h = int(size.find('height').text)\n",
    "\n",
    "    for obj in root.iter('object'):\n",
    "        cls = obj.find('name').text\n",
    "        print(cls)\n",
    "        if cls not in classes:\n",
    "            continue\n",
    "        cls_id = classes.index(cls)\n",
    "        xmlbox = obj.find('bndbox')\n",
    "        b = (float(xmlbox.find('xmin').text), float(xmlbox.find('xmax').text), float(xmlbox.find('ymin').text),\n",
    "             float(xmlbox.find('ymax').text))\n",
    "        bb = convert((w, h), b)\n",
    "        out_file.write(str(cls_id) + \" \" + \" \".join([str(a) for a in bb]) + '\\n')\n",
    "\n",
    "xml_path = './label_xml/'  # xml_path应该是上述步骤OBS桶文件夹C中的所有文件，记得拷贝过来\n",
    "\n",
    "img_xmls = os.listdir(xml_path)\n",
    "for img_xml in img_xmls:\n",
    "    label_name = img_xml.split('.')[0]\n",
    "    if  img_xml.split('.')[1] == 'xml':\n",
    "        convert_annotation(label_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06eccdf8-2e65-4c72-835e-48475592ad30",
   "metadata": {},
   "outputs": [],
   "source": [
    "'***转换xml标注文件为txt格式，无法直接运行***'     \n",
    "\n",
    "#随机选择300张标注完的各种keyframe（不一定来自于同一段视频）做测试集sample\n",
    "#随机选择300张标注完的各种keyframe（不一定来自于同一段视频）做验证集test\n",
    "#不被选为sample且不被选为test的全都归于训练集train\n",
    "\n",
    "#import moxing as mox\n",
    "from random import sample\n",
    "file_list = mox.file.list_directory( xml_path)  # xml_path中是上述步骤OBS桶文件夹C中的所有文件，记得拷贝到本地\n",
    "print(len(file_list))\n",
    "val_file_list = sample(file_list, 300)  # 选择了300张做测试集\n",
    "line = ''\n",
    "for i in val_file_list:\n",
    "    if i.endswith('.png') :\n",
    "        line += 'datasets/Fatigue_driving_detection/images/'+i+'\\n'     # datasets/Fatigue_driving_detection/images/ 是yolov7训练使用的\n",
    "with open('yolov7/datasets/Fatigue_driving_detection/val.txt', 'w+') as f:  \n",
    "    f.writelines(line)\n",
    "\n",
    "test_file_list = sample(file_list, 300)\n",
    "line = ''\n",
    "for i in test_file_list:\n",
    "    if i.endswith('.png'):\n",
    "        line += 'datasets/Fatigue_driving_detection/images/'+i+'\\n'\n",
    "with open('yolov7/datasets/Fatigue_driving_detection/test.txt', 'w+') as f:\n",
    "    f.writelines(line)\n",
    "\n",
    "line = ''\n",
    "for i in file_list:\n",
    "    if i not in val_file_list and i not in test_file_list:\n",
    "        if i.endswith('.png') :\n",
    "            line += 'datasets/Fatigue_driving_detection/images/'+i+'\\n'\n",
    "with open('yolov7/datasets/Fatigue_driving_detection/train.txt', 'w+') as f:\n",
    "    f.writelines(line)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e3f0c830-0790-4f8e-b34a-df75d67159cf",
   "metadata": {},
   "source": [
    "## 3、使用yolov7训练目标检测模型\n",
    "点击[此处](https://developer.huaweicloud.com/develop/aigallery/notebook/detail?id=4f59e6c9-3e48-48e7-a723-ac972bb85aa8)，直接运行即可"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ea6aaae0-d0c8-4815-8bd4-773fdad29fc3",
   "metadata": {},
   "source": [
    "## 4、结合视频实现疲劳/分神驾驶检测\n",
    "详细见如下代码<br>\n",
    "如果你第3步训练得到了更好的目标检测模型，将best.pt拷贝替换即可"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0690b57c-c658-41a8-9911-5769e44384db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 拷贝提供的baseline 模型包到本地\n",
    "#import moxing as mox\n",
    "#mox.file.copy_parallel('obs://obs-aigallery-zc/clf/model/video_classification','video_classification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3786fe8c-6c03-4505-8ce4-b1f57971fd17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 推理代码 展示\n",
    "from PIL import Image\n",
    "import copy\n",
    "import sys\n",
    "import traceback\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import cv2\n",
    "from input_reader import InputReader\n",
    "from tracker import Tracker\n",
    "from EAR import eye_aspect_ratio\n",
    "from MAR import mouth_aspect_ratio\n",
    "\n",
    "from models.experimental import attempt_load\n",
    "from utils1.general import check_img_size\n",
    "from tempfile import NamedTemporaryFile\n",
    "from utils1.torch_utils import TracedModel\n",
    "from detect import detect\n",
    "from model_service.pytorch_model_service import PTServingBaseService\n",
    "\n",
    "import threading\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "\n",
    "class fatigue_driving_detection(PTServingBaseService):\n",
    "    def __init__(self, model_name, model_path):\n",
    "        # these three parameters are no need to modify\n",
    "        self.model_name = model_name\n",
    "        self.model_path = model_path\n",
    "\n",
    "        self.capture = 'test.mp4'\n",
    "        self.result = 0\n",
    "        self.cancel_flag = threading.Event()\n",
    "\n",
    "        self.width = 1920\n",
    "        self.height = 1080\n",
    "        self.fps = 30\n",
    "        self.first = True\n",
    "\n",
    "        self.standard_pose = [180, 40, 80]\n",
    "        self.look_around_frame = 0\n",
    "        self.eyes_closed_frame = 0\n",
    "        self.mouth_open_frame = 0\n",
    "        self.use_phone_frame = 0\n",
    "        # lStart, lEnd) = (42, 48)\n",
    "        self.lStart = 42\n",
    "        self.lEnd = 48\n",
    "        # (rStart, rEnd) = (36, 42)\n",
    "        self.rStart = 36\n",
    "        self.rEnd = 42\n",
    "        # (mStart, mEnd) = (49, 66)\n",
    "        self.mStart = 49\n",
    "        self.mEnd = 66\n",
    "        self.EYE_AR_THRESH = 0.2\n",
    "        self.MOUTH_AR_THRESH = 0.6\n",
    "        self.frame_3s = self.fps * 3\n",
    "        self.face_detect = 0\n",
    "\n",
    "        self.weights = \"best.pt\"\n",
    "        self.imgsz = 640\n",
    "\n",
    "        self.device = 'cpu'  # 大赛后台使用CPU判分\n",
    "\n",
    "        model = attempt_load(model_path, map_location=self.device)\n",
    "        self.stride = int(model.stride.max())\n",
    "        self.imgsz = check_img_size(self.imgsz, s=self.stride)\n",
    "\n",
    "        self.model = TracedModel(model, self.device, self.imgsz)\n",
    "\n",
    "\n",
    "        self.need_reinit = 0\n",
    "        self.failures = 0\n",
    "\n",
    "        self.tracker = Tracker(self.width, self.height, threshold=None, max_threads=4, max_faces=4,\n",
    "                          discard_after=10, scan_every=3, silent=True, model_type=3,\n",
    "                          model_dir=None, no_gaze=False, detection_threshold=0.6,\n",
    "                          use_retinaface=0, max_feature_updates=900,\n",
    "                          static_model=True, try_hard=False)\n",
    "\n",
    "        # self.temp = NamedTemporaryFile(delete=False)  # 用来存储视频的临时文件\n",
    "\n",
    "    def _preprocess(self, data):\n",
    "        # preprocessed_data = {}\n",
    "        for k, v in data.items():\n",
    "            for file_name, file_content in v.items():\n",
    "                try:\n",
    "                    try:\n",
    "                        with open(self.capture, 'wb') as f:\n",
    "                            file_content_bytes = file_content.read()\n",
    "                            f.write(file_content_bytes)\n",
    "\n",
    "                    except Exception:\n",
    "                        return {\"message\": \"There was an error loading the file\"}\n",
    "\n",
    "                    # self.capture = self.temp.name  # Pass temp.name to VideoCapture()\n",
    "                except Exception:\n",
    "                    return {\"message\": \"There was an error processing the file\"}\n",
    "        return 'ok'\n",
    "    \n",
    "    # 检测是否转头\n",
    "    def checkLookAround(self,f):\n",
    "        while not self.cancel_flag.is_set():\n",
    "            if np.abs(self.standard_pose[0] - f.euler[0]) >= 45 or np.abs(self.standard_pose[1] - f.euler[1]) >= 45 or \\\n",
    "                np.abs(self.standard_pose[2] - f.euler[2]) >= 45:\n",
    "                self.look_around_frame += 1\n",
    "            else:\n",
    "                self.look_around_frame = 0\n",
    "            if self.look_around_frame >= self.frame_3s:\n",
    "                self.result = 4\n",
    "            return\n",
    "    \n",
    "    # 检测是否闭眼\n",
    "    def checkClosedEyes(self,f):\n",
    "        while not self.cancel_flag.is_set():\n",
    "            leftEye = f.lms[self.lStart:self.lEnd]\n",
    "            rightEye = f.lms[self.rStart:self.rEnd]\n",
    "            leftEAR = eye_aspect_ratio(leftEye)\n",
    "            rightEAR = eye_aspect_ratio(rightEye)\n",
    "            # average the eye aspect ratio together for both eyes\n",
    "            ear = (leftEAR + rightEAR) / 2.0\n",
    "            if ear < self.EYE_AR_THRESH:\n",
    "                self.eyes_closed_frame += 1\n",
    "            else:\n",
    "                self.eyes_closed_frame = 0\n",
    "                # print(ear, eyes_closed_frame)\n",
    "            if self.eyes_closed_frame >= self.frame_3s:\n",
    "                self.result = 1\n",
    "            return\n",
    "\n",
    "    # 检测是否张嘴\n",
    "    def checkMouthOpen(self,f):\n",
    "        while not self.cancel_flag.is_set():\n",
    "            mar = mouth_aspect_ratio(f.lms)\n",
    "            if mar > self.MOUTH_AR_THRESH:\n",
    "                self.mouth_open_frame += 1\n",
    "            if self.mouth_open_frame >= self.frame_3s:\n",
    "                self.result = 2\n",
    "            return\n",
    "        \n",
    "        \n",
    "\n",
    "    def _inference(self, data):\n",
    "        \"\"\"\n",
    "        model inference function\n",
    "        Here are a inference example of resnet, if you use another model, please modify this function\n",
    "        \"\"\"\n",
    "        print(data)\n",
    "        result = {\"result\": {\"category\": 0, \"duration\": 6000}}\n",
    "\n",
    "        self.input_reader = InputReader(self.capture, 0, self.width, self.height, self.fps)\n",
    "        source_name = self.input_reader.name\n",
    "        now = time.time()\n",
    "        while self.input_reader.is_open():\n",
    "\n",
    "            if not self.input_reader.is_open() or self.need_reinit == 1:\n",
    "                self.input_reader = InputReader(self.capture, 0, self.width, self.height, self.fps, use_dshowcapture=False, dcap=None)\n",
    "                if self.input_reader.name != source_name:\n",
    "                    print(f\"Failed to reinitialize camera and got {self.input_reader.name} instead of {source_name}.\")\n",
    "                    # sys.exit(1)\n",
    "                self.need_reinit = 2\n",
    "                time.sleep(0.02)\n",
    "                continue\n",
    "            if not self.input_reader.is_ready():\n",
    "                time.sleep(0.02)\n",
    "                continue\n",
    "\n",
    "            ret, frame = self.input_reader.read()\n",
    "\n",
    "            self.need_reinit = 0\n",
    "\n",
    "            try:\n",
    "                if frame is not None:\n",
    "                    # 剪裁主驾驶位\n",
    "                    frame = frame[:, 600:1920, :]\n",
    "\n",
    "                    # 检测驾驶员是否接打电话 以及低头的人脸\n",
    "                    bbox = detect(self.model, frame, self.stride, self.imgsz)\n",
    "                    # print(results)\n",
    "\n",
    "                    for box in bbox:\n",
    "                        if box[0] == 0:\n",
    "                            self.face_detect = 1\n",
    "                        if box[0] == 1:\n",
    "                            self.use_phone_frame += 1\n",
    "\n",
    "                    if self.use_phone_frame >= self.frame_3s:\n",
    "                        self.result = 3\n",
    "                        result['result']['category'] = 3\n",
    "                        break\n",
    "\n",
    "                    # 检测驾驶员是否张嘴、闭眼、转头\n",
    "                    faces = self.tracker.predict(frame)\n",
    "                    if len(faces) > 0:\n",
    "\n",
    "                        face_num = 0\n",
    "                        max_x = 0\n",
    "                        for face_num_index, f in enumerate(faces):\n",
    "                            if max_x <= f.bbox[3]:\n",
    "                                face_num = face_num_index\n",
    "                                max_x = f.bbox[3]\n",
    "\n",
    "\n",
    "\n",
    "                        f = faces[face_num]\n",
    "                        f = copy.copy(f)\n",
    "\n",
    "                        pool = ThreadPoolExecutor(3)\n",
    "\n",
    "                        lookAround = pool.submit(self.checkLookAround,self,f)\n",
    "                        mouthOpen = pool.submit(self.checkMouthOpen,self,f)\n",
    "                        eyeClosed = pool.submit(self.checkClosedEyes,self,f)\n",
    "\n",
    "                        pool.shutdown()\n",
    "\n",
    "                        while not self.result:\n",
    "                            if (lookAround.done() and mouthOpen.done() and eyeClosed.done()):\n",
    "                                break\n",
    "\n",
    "                        if self.result:\n",
    "                            self.cancel_flag.set()\n",
    "                            result['result']['category'] = self.result\n",
    "                            break\n",
    "\n",
    "                    else:\n",
    "                        if self.face_detect:\n",
    "                            self.look_around_frame += 1\n",
    "                            self.face_detect = 0\n",
    "                            if self.look_around_frame >= self.frame_3s:\n",
    "                                result['result']['category'] = 4\n",
    "                                break\n",
    "                    \n",
    "\n",
    "                    self.failures = 0\n",
    "                else:\n",
    "                    break\n",
    "            except Exception as e:\n",
    "                if e.__class__ == KeyboardInterrupt:\n",
    "                    print(\"Quitting\")\n",
    "                    break\n",
    "                traceback.print_exc()\n",
    "                self.failures += 1\n",
    "                if self.failures > 30:   # 失败超过30次就默认返回\n",
    "                    break\n",
    "            del frame\n",
    "        final_time = time.time()\n",
    "        duration = int(np.round((final_time - now) * 1000))\n",
    "        result['result']['duration'] = duration\n",
    "        return result\n",
    "\n",
    "    def _postprocess(self, data):\n",
    "        os.remove(self.capture)\n",
    "        return data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9511d5e1-7937-49a7-8f93-2960d04d6e50",
   "metadata": {},
   "source": [
    "将模型导入到ModelArts,这里使用了ModelArts的SDK，你也可以在ModelArts控制台界面直接导入模型<br>\n",
    "完成这一步，可以点击[此处](https://console.huaweicloud.com/modelarts/?region=cn-north-4#/myModel)，前往AI应用管理界面提交模型，如果还需要进行测试，可以部署在线服务进行测试<br>\n",
    "导入需要几分钟时间，请耐心等待"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3184d50d-ac93-49ac-8043-bf55fa95fada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://repo.myhuaweicloud.com/repository/pypi/simple\n",
      "\n",
      "\n",
      "\n",
      "Collecting json5\n",
      "\n",
      "\n",
      "\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/75/8c/c6f242154ee057e8f5b9491ee2a095646e489fcbad18dd73b99ed88cc5b2/json5-0.9.10-py2.py3-none-any.whl (19 kB)\n",
      "\n",
      "\n",
      "\n",
      "Installing collected packages: json5\n",
      "\n",
      "\n",
      "\n",
      "Successfully installed json5-0.9.10\n",
      "\n",
      "\n",
      "\n",
      "\u001b[33mWARNING: You are using pip version 21.0.1; however, version 23.0.1 is available.\n",
      "\n",
      "\n",
      "\n",
      "You should consider upgrading via the '/home/ma-user/anaconda3/envs/PyTorch-1.8/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "正在导入模型,模型名称： video_classification_900\n",
      "\n",
      "\n",
      "\n",
      "modelarts-cn-north-4-72b6beb2 is existed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Successfully upload file /home/ma-user/work/ma_share/Fatigue_driving_detection/video_classification/model/.ipynb_checkpoints to OBS modelarts-cn-north-4-72b6beb2/model-0331-020147/model\n",
      "\n",
      "\n",
      "\n",
      "Successfully upload file /home/ma-user/work/ma_share/Fatigue_driving_detection/video_classification/model to OBS modelarts-cn-north-4-72b6beb2/model-0331-020147\n",
      "\n",
      "\n",
      "\n",
      "Successfully upload file /home/ma-user/work/ma_share/Fatigue_driving_detection/video_classification/model to OBS modelarts-cn-north-4-72b6beb2/model-0331-020147\n",
      "\n",
      "\n",
      "\n",
      "Successfully upload file /home/ma-user/work/ma_share/Fatigue_driving_detection/video_classification/model to OBS modelarts-cn-north-4-72b6beb2/model-0331-020147\n",
      "\n",
      "\n",
      "\n",
      "Successfully upload file /home/ma-user/work/ma_share/Fatigue_driving_detection/video_classification/model to OBS modelarts-cn-north-4-72b6beb2/model-0331-020147\n",
      "\n",
      "\n",
      "\n",
      "Successfully upload file /home/ma-user/work/ma_share/Fatigue_driving_detection/video_classification/model to OBS modelarts-cn-north-4-72b6beb2/model-0331-020147\n",
      "\n",
      "\n",
      "\n",
      "Successfully upload file /home/ma-user/work/ma_share/Fatigue_driving_detection/video_classification/model to OBS modelarts-cn-north-4-72b6beb2/model-0331-020147\n",
      "\n",
      "\n",
      "\n",
      "Successfully upload file /home/ma-user/work/ma_share/Fatigue_driving_detection/video_classification/model to OBS modelarts-cn-north-4-72b6beb2/model-0331-020147\n",
      "\n",
      "\n",
      "\n",
      "Successfully upload file /home/ma-user/work/ma_share/Fatigue_driving_detection/video_classification/model to OBS modelarts-cn-north-4-72b6beb2/model-0331-020147\n",
      "\n",
      "\n",
      "\n",
      "Successfully upload file /home/ma-user/work/ma_share/Fatigue_driving_detection/video_classification/model to OBS modelarts-cn-north-4-72b6beb2/model-0331-020147\n",
      "\n",
      "\n",
      "\n",
      "Successfully upload file /home/ma-user/work/ma_share/Fatigue_driving_detection/video_classification/model to OBS modelarts-cn-north-4-72b6beb2/model-0331-020147\n",
      "\n",
      "\n",
      "\n",
      "Successfully upload file /home/ma-user/work/ma_share/Fatigue_driving_detection/video_classification/model/models to OBS modelarts-cn-north-4-72b6beb2/model-0331-020147/model\n",
      "\n",
      "\n",
      "\n",
      "Successfully upload file /home/ma-user/work/ma_share/Fatigue_driving_detection/video_classification/model/models to OBS modelarts-cn-north-4-72b6beb2/model-0331-020147/model\n",
      "\n",
      "\n",
      "\n",
      "Successfully upload file /home/ma-user/work/ma_share/Fatigue_driving_detection/video_classification/model/models to OBS modelarts-cn-north-4-72b6beb2/model-0331-020147/model\n",
      "\n",
      "\n",
      "\n",
      "Successfully upload file /home/ma-user/work/ma_share/Fatigue_driving_detection/video_classification/model/models to OBS modelarts-cn-north-4-72b6beb2/model-0331-020147/model\n",
      "\n",
      "\n",
      "\n",
      "Successfully upload file /home/ma-user/work/ma_share/Fatigue_driving_detection/video_classification/model/models to OBS modelarts-cn-north-4-72b6beb2/model-0331-020147/model\n",
      "\n",
      "\n",
      "\n",
      "Successfully upload file /home/ma-user/work/ma_share/Fatigue_driving_detection/video_classification/model/models to OBS modelarts-cn-north-4-72b6beb2/model-0331-020147/model\n",
      "\n",
      "\n",
      "\n",
      "Successfully upload file /home/ma-user/work/ma_share/Fatigue_driving_detection/video_classification/model/models to OBS modelarts-cn-north-4-72b6beb2/model-0331-020147/model\n",
      "\n",
      "\n",
      "\n",
      "Successfully upload file /home/ma-user/work/ma_share/Fatigue_driving_detection/video_classification/model/models to OBS modelarts-cn-north-4-72b6beb2/model-0331-020147/model\n",
      "\n",
      "\n",
      "\n",
      "Successfully upload file /home/ma-user/work/ma_share/Fatigue_driving_detection/video_classification/model/models to OBS modelarts-cn-north-4-72b6beb2/model-0331-020147/model\n",
      "\n",
      "\n",
      "\n",
      "Successfully upload file /home/ma-user/work/ma_share/Fatigue_driving_detection/video_classification/model/models to OBS modelarts-cn-north-4-72b6beb2/model-0331-020147/model\n",
      "\n",
      "\n",
      "\n",
      "Successfully upload file /home/ma-user/work/ma_share/Fatigue_driving_detection/video_classification/model/models to OBS modelarts-cn-north-4-72b6beb2/model-0331-020147/model\n",
      "\n",
      "\n",
      "\n",
      "Successfully upload file /home/ma-user/work/ma_share/Fatigue_driving_detection/video_classification/model/models to OBS modelarts-cn-north-4-72b6beb2/model-0331-020147/model\n",
      "\n",
      "\n",
      "\n",
      "Successfully upload file /home/ma-user/work/ma_share/Fatigue_driving_detection/video_classification/model/models to OBS modelarts-cn-north-4-72b6beb2/model-0331-020147/model\n",
      "\n",
      "\n",
      "\n",
      "Successfully upload file /home/ma-user/work/ma_share/Fatigue_driving_detection/video_classification/model/models to OBS modelarts-cn-north-4-72b6beb2/model-0331-020147/model\n",
      "\n",
      "\n",
      "\n",
      "Successfully upload file /home/ma-user/work/ma_share/Fatigue_driving_detection/video_classification/model/models to OBS modelarts-cn-north-4-72b6beb2/model-0331-020147/model\n",
      "\n",
      "\n",
      "\n",
      "Successfully upload file /home/ma-user/work/ma_share/Fatigue_driving_detection/video_classification/model/models to OBS modelarts-cn-north-4-72b6beb2/model-0331-020147/model\n",
      "\n",
      "\n",
      "\n",
      "Successfully upload file /home/ma-user/work/ma_share/Fatigue_driving_detection/video_classification/model/models to OBS modelarts-cn-north-4-72b6beb2/model-0331-020147/model\n",
      "\n",
      "\n",
      "\n",
      "Successfully upload file /home/ma-user/work/ma_share/Fatigue_driving_detection/video_classification/model to OBS modelarts-cn-north-4-72b6beb2/model-0331-020147\n",
      "\n",
      "\n",
      "\n",
      "Successfully upload file /home/ma-user/work/ma_share/Fatigue_driving_detection/video_classification/model to OBS modelarts-cn-north-4-72b6beb2/model-0331-020147\n",
      "\n",
      "\n",
      "\n",
      "Successfully upload file /home/ma-user/work/ma_share/Fatigue_driving_detection/video_classification/model to OBS modelarts-cn-north-4-72b6beb2/model-0331-020147\n",
      "\n",
      "\n",
      "\n",
      "Successfully upload file /home/ma-user/work/ma_share/Fatigue_driving_detection/video_classification/model/utils1 to OBS modelarts-cn-north-4-72b6beb2/model-0331-020147/model\n",
      "\n",
      "\n",
      "\n",
      "Successfully upload file /home/ma-user/work/ma_share/Fatigue_driving_detection/video_classification/model/utils1 to OBS modelarts-cn-north-4-72b6beb2/model-0331-020147/model\n",
      "\n",
      "\n",
      "\n",
      "Successfully upload file /home/ma-user/work/ma_share/Fatigue_driving_detection/video_classification/model/utils1 to OBS modelarts-cn-north-4-72b6beb2/model-0331-020147/model\n",
      "\n",
      "\n",
      "\n",
      "Successfully upload file /home/ma-user/work/ma_share/Fatigue_driving_detection/video_classification/model/utils1 to OBS modelarts-cn-north-4-72b6beb2/model-0331-020147/model\n",
      "\n",
      "\n",
      "\n",
      "Successfully upload file /home/ma-user/work/ma_share/Fatigue_driving_detection/video_classification/model/utils1 to OBS modelarts-cn-north-4-72b6beb2/model-0331-020147/model\n",
      "\n",
      "\n",
      "\n",
      "Successfully upload file /home/ma-user/work/ma_share/Fatigue_driving_detection/video_classification/model/utils1 to OBS modelarts-cn-north-4-72b6beb2/model-0331-020147/model\n",
      "\n",
      "\n",
      "\n",
      "Successfully upload file /home/ma-user/work/ma_share/Fatigue_driving_detection/video_classification/model/utils1 to OBS modelarts-cn-north-4-72b6beb2/model-0331-020147/model\n",
      "\n",
      "\n",
      "\n",
      "Successfully upload file /home/ma-user/work/ma_share/Fatigue_driving_detection/video_classification/model/utils1 to OBS modelarts-cn-north-4-72b6beb2/model-0331-020147/model\n",
      "\n",
      "\n",
      "\n",
      "Successfully upload file /home/ma-user/work/ma_share/Fatigue_driving_detection/video_classification/model/utils1 to OBS modelarts-cn-north-4-72b6beb2/model-0331-020147/model\n",
      "\n",
      "\n",
      "\n",
      "Successfully upload file /home/ma-user/work/ma_share/Fatigue_driving_detection/video_classification/model/utils1 to OBS modelarts-cn-north-4-72b6beb2/model-0331-020147/model\n",
      "\n",
      "\n",
      "\n",
      "Successfully upload file /home/ma-user/work/ma_share/Fatigue_driving_detection/video_classification/model/utils1/wandb_logging to OBS modelarts-cn-north-4-72b6beb2/model-0331-020147/model/utils1\n",
      "\n",
      "\n",
      "\n",
      "Successfully upload file /home/ma-user/work/ma_share/Fatigue_driving_detection/video_classification/model/utils1/wandb_logging to OBS modelarts-cn-north-4-72b6beb2/model-0331-020147/model/utils1\n",
      "\n",
      "\n",
      "\n",
      "Successfully upload file /home/ma-user/work/ma_share/Fatigue_driving_detection/video_classification/model/utils1/wandb_logging to OBS modelarts-cn-north-4-72b6beb2/model-0331-020147/model/utils1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully upload file /home/ma-user/work/ma_share/Fatigue_driving_detection/video_classification/model to OBS modelarts-cn-north-4-72b6beb2/model-0331-020147\n",
      "\n",
      "\n",
      "\n",
      "Successfully upload model files from /home/ma-user/work/ma_share/Fatigue_driving_detection/video_classification/model to obs path /modelarts-cn-north-4-72b6beb2/model-0331-020147.\n",
      "\n",
      "\n",
      "\n",
      "The model source location is https://modelarts-cn-north-4-72b6beb2.obs.cn-north-4.myhuaweicloud.com/model-0331-020147/model\n",
      "\n",
      "\n",
      "\n",
      "publishing\n",
      "\n",
      "\n",
      "\n",
      "published\n",
      "\n",
      "\n",
      "\n",
      "所有模型导入完成\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "from modelarts.session import Session\n",
    "from modelarts.model import Model\n",
    "from modelarts.config.model_config import TransformerConfig,Params\n",
    "#!pip install json5\n",
    "import json5\n",
    "import re\n",
    "import traceback\n",
    "import random\n",
    "\n",
    "try:\n",
    "    session = Session()\n",
    "    config_path = 'video_classification/model/config.json' \n",
    "    if mox.file.exists(config_path):                                        # 判断一下是否存在配置文件，如果没有则不能导入模型\n",
    "        model_location =  'video_classification/model'\n",
    "        model_name = \"video_classification\"\n",
    "        load_dict = json5.loads(mox.file.read(config_path))\n",
    "        model_type = load_dict['model_type']\n",
    "        re_name = '_'+str(random.randint(0,1000))\n",
    "        model_name += re_name\n",
    "        print(\"正在导入模型,模型名称：\", model_name)\n",
    "        model_instance = Model(\n",
    "                     session, \n",
    "                     model_name=model_name,               # 模型名称\n",
    "                     model_version=\"1.0.0\",               # 模型版本\n",
    "                      source_location_type='LOCAL_SOURCE',\n",
    "                     source_location=model_location,      # 模型文件路径\n",
    "                     model_type=model_type,               # 模型类型\n",
    "                     )\n",
    "\n",
    "    print(\"所有模型导入完成\")\n",
    "except Exception as e:\n",
    "    print(\"发生了一些问题，请看下面的报错信息：\") \n",
    "    traceback.print_exc()\n",
    "    print(\"模型导入失败\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f1d770-2f17-46ab-98bd-a9b9116e19dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "AIGalleryInfo": "",
  "flavorInfo": {
   "architecture": "X86_64",
   "category": "CPU"
  },
  "imageInfo": {
   "id": "278e88d1-5b71-4766-8502-b3ba72e824d9",
   "name": "pytorch1.8-cuda10.2-cudnn7-ubuntu18.04"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
